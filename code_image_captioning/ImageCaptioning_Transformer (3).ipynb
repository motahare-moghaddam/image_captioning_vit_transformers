{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ja1K4gBbzn3"
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AldVDvOgcpbc"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "import requests\n",
    "from math import sqrt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'images'\n",
    "WORKING_DIR = 'imag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv(file_path, output_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Handle lines with more columns than expected\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        columns = line.split(',')\n",
    "        if len(columns) == 10:\n",
    "            cleaned_lines.append(line)\n",
    "        # Optionally handle lines with more columns\n",
    "        elif len(columns) > 10:\n",
    "            cleaned_lines.append(','.join(columns[:10]) + '\\n')\n",
    "\n",
    "    with open(output_path, 'w') as file:\n",
    "        file.writelines(cleaned_lines)\n",
    "\n",
    "clean_csv('imag/styles.csv', 'imag/styles_clean.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Agz2DaoDbXZp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 6044: expected 10 fields, saw 11\n",
      "Skipping line 6569: expected 10 fields, saw 11\n",
      "Skipping line 7399: expected 10 fields, saw 11\n",
      "Skipping line 7939: expected 10 fields, saw 11\n",
      "Skipping line 9026: expected 10 fields, saw 11\n",
      "Skipping line 10264: expected 10 fields, saw 11\n",
      "Skipping line 10427: expected 10 fields, saw 11\n",
      "Skipping line 10905: expected 10 fields, saw 11\n",
      "Skipping line 11373: expected 10 fields, saw 11\n",
      "Skipping line 11945: expected 10 fields, saw 11\n",
      "Skipping line 14112: expected 10 fields, saw 11\n",
      "Skipping line 14532: expected 10 fields, saw 11\n",
      "Skipping line 15076: expected 10 fields, saw 12\n",
      "Skipping line 29906: expected 10 fields, saw 11\n",
      "Skipping line 31625: expected 10 fields, saw 11\n",
      "Skipping line 33020: expected 10 fields, saw 11\n",
      "Skipping line 35748: expected 10 fields, saw 11\n",
      "Skipping line 35962: expected 10 fields, saw 11\n",
      "Skipping line 37770: expected 10 fields, saw 11\n",
      "Skipping line 38105: expected 10 fields, saw 11\n",
      "Skipping line 38275: expected 10 fields, saw 11\n",
      "Skipping line 38404: expected 10 fields, saw 12\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id gender masterCategory subCategory  articleType baseColour  season  \\\n",
      "0  15970    Men        Apparel     Topwear       Shirts  Navy Blue    Fall   \n",
      "1  39386    Men        Apparel  Bottomwear        Jeans       Blue  Summer   \n",
      "2  59263  Women    Accessories     Watches      Watches     Silver  Winter   \n",
      "3  21379    Men        Apparel  Bottomwear  Track Pants      Black    Fall   \n",
      "4  53759    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
      "\n",
      "     year   usage                             productDisplayName  \n",
      "0  2011.0  Casual               Turtle Check Men Navy Blue Shirt  \n",
      "1  2012.0  Casual             Peter England Men Party Blue Jeans  \n",
      "2  2016.0  Casual                       Titan Women Silver Watch  \n",
      "3  2011.0  Casual  Manchester United Men Solid Black Track Pants  \n",
      "4  2012.0  Casual                          Puma Men Grey T-shirt  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 44424/44424 [00:05<00:00, 7887.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] boys apparel bottomwear capris blue summer casual gini and jony boys washed blue 34 length pants [end]\n",
      "Image not found: images\\39403.jpg\n",
      "Image not found: images\\39410.jpg\n",
      "Image not found: images\\39401.jpg\n",
      "Image not found: images\\39425.jpg\n",
      "Image not found: images\\12347.jpg\n",
      "Found 44419 valid image paths out of 44424 unique images.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Load the styles.csv file with error handling\n",
    "try:\n",
    "    styles_df = pd.read_csv('imag/styles.csv', on_bad_lines='warn')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"ParserError: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Check if styles_df was defined successfully\n",
    "if 'styles_df' in locals():\n",
    "    print(styles_df.head())\n",
    "else:\n",
    "    print(\"Failed to load the DataFrame.\")\n",
    "\n",
    "# Define the label columns\n",
    "label_columns = ['gender', 'masterCategory', 'subCategory', 'articleType', 'baseColour', 'season', 'usage', 'productDisplayName']\n",
    "\n",
    "# Initialize a mapping dictionary\n",
    "mapping = {}\n",
    "\n",
    "# Iterate through the DataFrame rows\n",
    "for index, row in tqdm(styles_df.iterrows(), total=styles_df.shape[0]):\n",
    "    image_id = row['id']  # Assuming 'id' is the column name for image ID\n",
    "    caption = \" \".join(str(row[col]) for col in label_columns if col in row and pd.notna(row[col]))\n",
    "    \n",
    "    # Clean the image_id if needed (e.g., remove file extension)\n",
    "    image_id = str(image_id).split('.')[0]  # Adjust based on your image ID format\n",
    "    \n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    mapping[image_id].append(caption)\n",
    "\n",
    "# Convert mapping to DataFrame\n",
    "captions = pd.DataFrame([(img_id, cap) for img_id, caps in mapping.items() for cap in caps], columns=['image', 'caption'])\n",
    "\n",
    "# Preprocess captions\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = '[start] ' + text + ' [end]'\n",
    "    return text\n",
    "\n",
    "captions['caption'] = captions['caption'].apply(preprocess)\n",
    "\n",
    "# Display a random caption\n",
    "random_row = captions.sample(1).iloc[0]\n",
    "print(random_row.caption)\n",
    "\n",
    "# Construct image paths and verify they exist\n",
    "image_dir = 'images'  # Set this to the actual path to your image directory\n",
    "valid_image_paths = []\n",
    "\n",
    "for img_id in captions['image'].unique():\n",
    "    img_path = os.path.join(image_dir, f\"{img_id}.jpg\")  # Adjust extension if needed\n",
    "    if os.path.exists(img_path):\n",
    "        valid_image_paths.append(img_path)\n",
    "    else:\n",
    "        print(f\"Image not found: {img_path}\")\n",
    "\n",
    "print(f\"Found {len(valid_image_paths)} valid image paths out of {len(captions['image'].unique())} unique images.\")\n",
    "\n",
    "# Continue with valid_image_paths for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WOjpQCugErhb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] start women apparel topwear tops multi summer casual tonga women multicoloured top end [end]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess captions\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = '[start] ' + text + ' [end]'\n",
    "    return text\n",
    "\n",
    "captions['caption'] = captions['caption'].apply(preprocess)\n",
    "\n",
    "# Display a random caption\n",
    "random_row = captions.sample(1).iloc[0]\n",
    "print(random_row.caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6RBuExHWnGEt"
   },
   "outputs": [],
   "source": [
    "# Assuming your images are in .jpg format\n",
    "image_file_path = f'C:/Users/user/images/{random_row.image}.jpg'  # Add the .jpg extension\n",
    "im = Image.open(image_file_path)\n",
    "im.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nSTivH_FSSf2"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "VOCABULARY_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "EMBEDDING_DIM = 512\n",
    "UNITS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "X8MGUNtBN2sz"
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCABULARY_SIZE,\n",
    "    standardize=None,\n",
    "    output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "tokenizer.adapt(captions['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "qvhg-6eKN3nz"
   },
   "outputs": [],
   "source": [
    "word2idx = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary())\n",
    "\n",
    "idx2word = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary(),\n",
    "    invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Yrca2aN2N5WL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Define the base directory for your images\n",
    "image_dir = 'images'  # Adjust this path to where your images are located\n",
    "\n",
    "# Create a dictionary to map images to captions\n",
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(captions['image'], captions['caption']):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# Shuffle the image keys\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "# Split keys into training and validation sets\n",
    "slice_index = int(len(img_keys) * 0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "# Create lists for training images and captions with full paths\n",
    "train_imgs = []\n",
    "train_captions = []\n",
    "for imgt in img_name_train_keys:\n",
    "    # Append the full path for training images\n",
    "    full_path = os.path.join(image_dir, f\"{imgt}.jpg\")  # Adjust the extension if needed\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    train_imgs.extend([full_path] * capt_len)\n",
    "    train_captions.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "# Create lists for validation images and captions with full paths\n",
    "val_imgs = []\n",
    "val_captions = []\n",
    "for imgv in img_name_val_keys:\n",
    "    # Append the full path for validation images\n",
    "    full_path = os.path.join(image_dir, f\"{imgv}.jpg\")  # Adjust the extension if needed\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    val_imgs.extend([full_path] * capv_len)\n",
    "    val_captions.extend(img_to_cap_vector[imgv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UHN3Q1YDN5TD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35539, 35539, 8885, 8885)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_imgs), len(train_captions), len(val_imgs), len(val_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_path, caption):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [299, 299])  # Use tf.image.resize instead of tf.keras.layers.Resizing\n",
    "    img = img / 255.0  # Normalize the image\n",
    "    caption = tokenizer(caption)  # Ensure tokenizer is properly defined\n",
    "    return img, caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_imgs, train_captions))\n",
    "train_dataset = train_dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_imgs, val_captions))\n",
    "val_dataset = val_dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing paths: ['images\\\\39401.jpg', 'images\\\\39403.jpg', 'images\\\\39410.jpg', 'images\\\\12347.jpg', 'images\\\\39425.jpg']\n",
      "All paths are valid.\n"
     ]
    }
   ],
   "source": [
    "# Verify if all image paths exist\n",
    "import os\n",
    "\n",
    "def check_image_paths(img_paths):\n",
    "    missing_paths = [path for path in img_paths if not os.path.exists(path)]\n",
    "    if missing_paths:\n",
    "        print(f\"Missing paths: {missing_paths}\")\n",
    "    else:\n",
    "        print(\"All paths are valid.\")\n",
    "\n",
    "check_image_paths(train_imgs)\n",
    "check_image_paths(val_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_path, caption):\n",
    "    try:\n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.io.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [299, 299])  # Use tf.image.resize instead of tf.keras.layers.Resizing\n",
    "        img = img / 255.0  # Normalize the image\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {img_path}: {e}\")\n",
    "        img = tf.zeros([299, 299, 3])  # Return a placeholder image if an error occurs\n",
    "    caption = tokenizer(caption)  # Ensure tokenizer is properly defined\n",
    "    return img, caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out missing image paths\n",
    "def filter_missing_paths(img_paths, captions):\n",
    "    existing_paths = [path for path in img_paths if os.path.exists(path)]\n",
    "    filtered_captions = [caption for path, caption in zip(img_paths, captions) if os.path.exists(path)]\n",
    "    return existing_paths, filtered_captions\n",
    "\n",
    "train_imgs, train_captions = filter_missing_paths(train_imgs, train_captions)\n",
    "val_imgs, val_captions = filter_missing_paths(val_imgs, val_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_imgs, train_captions))\n",
    "train_dataset = train_dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_imgs, val_captions))\n",
    "val_dataset = val_dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All paths are valid.\n",
      "All paths are valid.\n"
     ]
    }
   ],
   "source": [
    "# Verify if all image paths exist\n",
    "import os\n",
    "\n",
    "def check_image_paths(img_paths):\n",
    "    missing_paths = [path for path in img_paths if not os.path.exists(path)]\n",
    "    if missing_paths:\n",
    "        print(f\"Missing paths: {missing_paths}\")\n",
    "    else:\n",
    "        print(\"All paths are valid.\")\n",
    "\n",
    "check_image_paths(train_imgs)\n",
    "check_image_paths(val_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: (32, 299, 299, 3)\n",
      "Caption batch shape: (32, 40)\n"
     ]
    }
   ],
   "source": [
    "for img_batch, caption_batch in train_dataset.take(1):\n",
    "    print(f\"Image batch shape: {img_batch.shape}\")\n",
    "    print(f\"Caption batch shape: {caption_batch.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "vHk83y3eOFPz"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_imgs, train_captions))\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_imgs, val_captions))\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "bQr_bgk11eMF"
   },
   "outputs": [],
   "source": [
    "image_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        tf.keras.layers.RandomContrast(0.3),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def CNN_Encoder():\n",
    "    # Load the InceptionV3 model with custom weights\n",
    "    inception_v3 = tf.keras.applications.InceptionV3(\n",
    "        include_top=False,\n",
    "        weights='inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',  # Update this path\n",
    "    )\n",
    "    inception_v3.trainable = False\n",
    "\n",
    "    output = inception_v3.output\n",
    "    output = tf.keras.layers.Reshape((-1, output.shape[-1]))(output)\n",
    "\n",
    "    cnn_model = tf.keras.models.Model(inception_v3.input, output)\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "H9GDJ9_1nIMO"
   },
   "outputs": [],
   "source": [
    "def CNN_Encoder():\n",
    "    inception_v3 = tf.keras.applications.InceptionV3(\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    inception_v3.trainable = False\n",
    "\n",
    "    output = inception_v3.output\n",
    "    output = tf.keras.layers.Reshape(\n",
    "        (-1, output.shape[-1]))(output)\n",
    "\n",
    "    cnn_model = tf.keras.models.Model(inception_v3.input, output)\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jMy5MrE2PdHV"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embed_dim, activation=\"relu\")\n",
    "\n",
    "\n",
    "    def call(self, x, training):\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        attn_output = self.attention(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            attention_mask=None,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        x = self.layer_norm_2(x + attn_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFqNFts0duGB"
   },
   "outputs": [],
   "source": [
    "class Embeddings(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(\n",
    "            vocab_size, embed_dim)\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            max_len, embed_dim, input_shape=(None, max_len))\n",
    "\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        length = tf.shape(input_ids)[-1]\n",
    "        position_ids = tf.range(start=0, limit=length, delta=1)\n",
    "        position_ids = tf.expand_dims(position_ids, axis=0)\n",
    "\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7QfeQ1XbXWE"
   },
   "outputs": [],
   "source": [
    "Embeddings(tokenizer.vocabulary_size(), EMBEDDING_DIM, MAX_LENGTH)(next(iter(train_dataset))[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are present.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_dir = 'images'  # Adjust this path as needed\n",
    "\n",
    "# List all image files in the directory\n",
    "image_files = os.listdir(image_dir)\n",
    "\n",
    "# Print files that are missing or check for specific missing files\n",
    "missing_files = [f for f in image_files if not os.path.isfile(os.path.join(image_dir, f))]\n",
    "if missing_files:\n",
    "    print(f\"Missing files: {missing_files}\")\n",
    "else:\n",
    "    print(\"All files are present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\\44712.jpg\n",
      "images\\10325.jpg\n",
      "images\\10176.jpg\n",
      "images\\39945.jpg\n",
      "images\\10997.jpg\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the file path pattern\n",
    "file_pattern = 'images/*.jpg'  # Ensure this pattern is correct\n",
    "\n",
    "# Create dataset\n",
    "dataset = tf.data.Dataset.list_files(file_pattern)\n",
    "\n",
    "# Print file paths in the dataset\n",
    "for file_path in dataset.take(5):  # Check first 5 paths\n",
    "    print(file_path.numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(file_path):\n",
    "    try:\n",
    "        image = tf.io.read_file(file_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, [224, 224])\n",
    "        image = image / 255.0\n",
    "        return image\n",
    "    except tf.errors.NotFoundError:\n",
    "        print(f\"File not found: {file_path.numpy().decode('utf-8')}\")\n",
    "        return tf.zeros([224, 224, 3])  # Return a default image or handle the error as needed\n",
    "\n",
    "# Map function over dataset\n",
    "dataset = tf.data.Dataset.list_files(file_pattern)\n",
    "dataset = dataset.map(lambda x: parse_image(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (224, 224, 3)\n",
      "Image shape: (224, 224, 3)\n",
      "Image shape: (224, 224, 3)\n",
      "Image shape: (224, 224, 3)\n",
      "Image shape: (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "try:\n",
    "    for i in range(5):  # Try fetching 5 elements\n",
    "        image = next(iterator)\n",
    "        print(f\"Image shape: {image.shape}\")\n",
    "except StopIteration:\n",
    "    print(\"No more elements in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: (32, 224, 224, 3)\n",
      "Batch shape: (32, 224, 224, 3)\n",
      "Batch shape: (32, 224, 224, 3)\n",
      "Batch shape: (32, 224, 224, 3)\n",
      "Batch shape: (32, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Path to the image directory\n",
    "image_dir = 'images'\n",
    "file_pattern = os.path.join(image_dir, '*.jpg')\n",
    "\n",
    "def parse_image(file_path):\n",
    "    try:\n",
    "        image = tf.io.read_file(file_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, [224, 224])\n",
    "        image = image / 255.0\n",
    "        return image\n",
    "    except tf.errors.NotFoundError:\n",
    "        # Handle file not found error\n",
    "        print(f\"File not found: {file_path.numpy().decode('utf-8')}\")\n",
    "        return tf.zeros([224, 224, 3])  # Return a default image or handle as needed\n",
    "\n",
    "# Create dataset\n",
    "dataset = tf.data.Dataset.list_files(file_pattern)\n",
    "\n",
    "# Map function over dataset\n",
    "dataset = dataset.map(lambda x: parse_image(x))\n",
    "\n",
    "# Batch and prefetch for performance\n",
    "dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Iterate over dataset\n",
    "iterator = iter(dataset)\n",
    "try:\n",
    "    for i in range(5):  # Try fetching 5 elements\n",
    "        images = next(iterator)\n",
    "        print(f\"Batch shape: {images.shape}\")\n",
    "except StopIteration:\n",
    "    print(\"No more elements in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read image: images\\6386.jpg\n",
      "Successfully read image: images\\29747.jpg\n",
      "Successfully read image: images\\5322.jpg\n",
      "Successfully read image: images\\40786.jpg\n",
      "Successfully read image: images\\4155.jpg\n",
      "Successfully read image: images\\9701.jpg\n",
      "Successfully read image: images\\21138.jpg\n",
      "Successfully read image: images\\9964.jpg\n",
      "Successfully read image: images\\2213.jpg\n",
      "Successfully read image: images\\10396.jpg\n"
     ]
    }
   ],
   "source": [
    "# Define a dataset with limited examples to test\n",
    "test_dataset = tf.data.Dataset.list_files(file_pattern).take(10)\n",
    "\n",
    "for file_path in test_dataset:\n",
    "    try:\n",
    "        image = parse_image(file_path)\n",
    "        print(f\"Successfully read image: {file_path.numpy().decode('utf-8')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading image: {file_path.numpy().decode('utf-8')}. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "pcbCQqrDnJ4-"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, units, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = Embeddings(\n",
    "            tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)\n",
    "\n",
    "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=\"softmax\")\n",
    "\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def call(self, input_ids, encoder_output, training, mask=None):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "\n",
    "        combined_mask = None\n",
    "        padding_mask = None\n",
    "\n",
    "        if mask is not None:\n",
    "            causal_mask = self.get_causal_attention_mask(embeddings)\n",
    "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "\n",
    "        attn_output_1 = self.attention_1(\n",
    "            query=embeddings,\n",
    "            value=embeddings,\n",
    "            key=embeddings,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        out_1 = self.layernorm_1(embeddings + attn_output_1)\n",
    "\n",
    "        attn_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_output,\n",
    "            key=encoder_output,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        out_2 = self.layernorm_2(out_1 + attn_output_2)\n",
    "\n",
    "        ffn_out = self.ffn_layer_1(out_2)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "        ffn_out = self.layernorm_3(ffn_out + out_2)\n",
    "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "        preds = self.out(ffn_out)\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "9_NmSUaVys9R"
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.image_aug = image_aug\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")\n",
    "\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        accuracy = tf.math.logical_and(mask, accuracy)\n",
    "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "    def compute_loss_and_acc(self, img_embed, captions, training=True):\n",
    "        encoder_output = self.encoder(img_embed, training=True)\n",
    "        y_input = captions[:, :-1]\n",
    "        y_true = captions[:, 1:]\n",
    "        mask = (y_true != 0)\n",
    "        y_pred = self.decoder(\n",
    "            y_input, encoder_output, training=True, mask=mask\n",
    "        )\n",
    "        loss = self.calculate_loss(y_true, y_pred, mask)\n",
    "        acc = self.calculate_accuracy(y_true, y_pred, mask)\n",
    "        return loss, acc\n",
    "\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        imgs, captions = batch\n",
    "\n",
    "        if self.image_aug:\n",
    "            imgs = self.image_aug(imgs)\n",
    "\n",
    "        img_embed = self.cnn_model(imgs)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, acc = self.compute_loss_and_acc(\n",
    "                img_embed, captions\n",
    "            )\n",
    "\n",
    "        train_vars = (\n",
    "            self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        )\n",
    "        grads = tape.gradient(loss, train_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        imgs, captions = batch\n",
    "\n",
    "        img_embed = self.cnn_model(imgs)\n",
    "\n",
    "        loss, acc = self.compute_loss_and_acc(\n",
    "            img_embed, captions, training=False\n",
    "        )\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.acc_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "GqWpcsje0Hkh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "encoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)\n",
    "decoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)\n",
    "#import h5py\n",
    "cnn_model = CNN_Encoder()\n",
    "caption_model = ImageCaptioningModel(\n",
    "    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "bayNssgNX6QN"
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=\"none\"\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "caption_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=cross_entropy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "1RYo-MRVYn49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m1111/1111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4122s\u001b[0m 4s/step - acc: 0.7756 - loss: 0.8742 - val_acc: 0.7659 - val_loss: 1.0225\n",
      "Epoch 2/2\n",
      "\u001b[1m1111/1111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3962s\u001b[0m 4s/step - acc: 0.7806 - loss: 0.8385 - val_acc: 0.7732 - val_loss: 0.9973\n"
     ]
    }
   ],
   "source": [
    "history = caption_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=2,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "h2u16yXW3-1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word(2).numpy().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "3ErlQQICtj_g"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_image_from_path(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
    "    img = tf.cast(img, tf.float32)  # Cast to float32 before division\n",
    "    img = img / 255.0  # Normalize the image\n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_caption(img_path):\n",
    "    img = load_image_from_path(img_path)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    img_embed = caption_model.cnn_model(img)\n",
    "    img_encoded = caption_model.encoder(img_embed, training=False)\n",
    "\n",
    "    y_inp = '[start]'\n",
    "    for i in range(MAX_LENGTH-1):\n",
    "        tokenized = tokenizer([y_inp])[:, :-1]\n",
    "        mask = tf.cast(tokenized != 0, tf.int32)\n",
    "        pred = caption_model.decoder(\n",
    "            tokenized, img_encoded, training=False, mask=mask)\n",
    "\n",
    "        pred_idx = np.argmax(pred[0, i, :])\n",
    "        pred_word = idx2word(pred_idx).numpy().decode('utf-8')\n",
    "        if pred_word == '[end]':\n",
    "            break\n",
    "\n",
    "        y_inp += ' ' + pred_word\n",
    "\n",
    "    y_inp = y_inp.replace('[start] ', '')\n",
    "    return y_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Caption: [start] start men footwear shoes formal shoes brown summer formal franco leone men brown formal shoes end [end]\n",
      "Predicted Caption: start men footwear shoes formal shoes brown summer formal franco leone men brown formal shoes end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select a random image\n",
    "idx = random.randrange(0, len(val_imgs))\n",
    "img_path = val_imgs[idx]\n",
    "\n",
    "# Generate predicted caption\n",
    "pred_caption = generate_caption(img_path)\n",
    "\n",
    "# Retrieve the actual caption\n",
    "actual_caption = val_captions[idx]  # Make sure this corresponds correctly to val_imgs\n",
    "\n",
    "# Display the captions and the image\n",
    "print('Actual Caption:', actual_caption)\n",
    "print('Predicted Caption:', pred_caption)\n",
    "print()\n",
    "Image.open(img_path).show()  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: start women apparel topwear tshirts pink summer casual myntra women pink tshirt end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_caption = generate_caption('test_for_fashion/w5.jpg')\n",
    "print('Predicted Caption:', pred_caption)\n",
    "print()\n",
    "#im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: start men footwear shoes sports shoes white summer sports nike men air impetus white sports shoes end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_caption = generate_caption('test_for_fashion/w9.jpg')\n",
    "print('Predicted Caption:', pred_caption)\n",
    "print()\n",
    "#im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: start men footwear shoes casual shoes brown fall casual rockport mens hydro sail brown blue shoe end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_caption = generate_caption('test_for_fashion/m10.jpg')\n",
    "print('Predicted Caption:', pred_caption)\n",
    "print()\n",
    "#im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: start women accessories jewellery jewellery set gold winter casual estelle women gold jewellery set end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_caption = generate_caption('test_for_fashion/w17.jpg')\n",
    "print('Predicted Caption:', pred_caption)\n",
    "print()\n",
    "#im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: start men accessories watches watches black winter casual casio gshock men black digital watch ex030 end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_caption = generate_caption('test_for_fashion/wat.jpg')\n",
    "print('Predicted Caption:', pred_caption)\n",
    "print()\n",
    "#im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Caption: start men accessories watches watches black winter casual casio gshock men black digital watch ex030 end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_caption = generate_caption('test_for_fashion/wat.jpg')\n",
    "print('Predicted Caption:', pred_caption)\n",
    "print()\n",
    "#im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTJdCYm4r0TJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Save the model architecture and weights\n",
    "model_path = 'image_captioning_transformer_model.h5'\n",
    "weights_path = 'image_captioning_transformer_weights.weights.h5'\n",
    "\n",
    "# Save the model architecture\n",
    "caption_model.save(model_path)\n",
    "\n",
    "# Save the model weights\n",
    "caption_model.save_weights(weights_path)\n",
    "\n",
    "# Create a directory for saving files if needed\n",
    "os.makedirs('Saved Models2', exist_ok=True)\n",
    "\n",
    "# Move the saved files to the created directory\n",
    "os.rename(model_path, f'Saved Models2/{model_path}')\n",
    "os.rename(weights_path, f'Saved Models2/{weights_path}')\n",
    "\n",
    "# Print paths\n",
    "print(f'Model saved to: Saved Models2/{model_path}')\n",
    "print(f'Weights saved to: Saved Models2/{weights_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG69m29gs6W4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define your model architecture\n",
    "# Make sure to replicate the architecture exactly as it was when you saved it\n",
    "\n",
    "encoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)\n",
    "decoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)\n",
    "#import h5py\n",
    "cnn_model = CNN_Encoder()\n",
    "caption_model = ImageCaptioningModel(\n",
    "    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n",
    ")\n",
    "caption_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=cross_entropy\n",
    ")\n",
    "# Load the model weights\n",
    "caption_model.load_weights('Saved Models2/image_captioning_transformer_weights.weights.h5')\n",
    "\n",
    "def load_image_from_path(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
    "    img = tf.cast(img, tf.float32)  # Cast to float32 before division\n",
    "    img = img / 255.0  # Normalize the image\n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_caption(img_path):\n",
    "    img = load_image_from_path(img_path)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    img_embed = caption_model.cnn_model(img)\n",
    "    img_encoded = caption_model.encoder(img_embed, training=False)\n",
    "\n",
    "    y_inp = '[start]'\n",
    "    for i in range(MAX_LENGTH-1):\n",
    "        tokenized = tokenizer([y_inp])[:, :-1]\n",
    "        mask = tf.cast(tokenized != 0, tf.int32)\n",
    "        pred = caption_model.decoder(\n",
    "            tokenized, img_encoded, training=False, mask=mask)\n",
    "\n",
    "        pred_idx = np.argmax(pred[0, i, :])\n",
    "        pred_word = idx2word(pred_idx).numpy().decode('utf-8')\n",
    "        if pred_word == '[end]':\n",
    "            break\n",
    "\n",
    "        y_inp += ' ' + pred_word\n",
    "\n",
    "    y_inp = y_inp.replace('[start] ', '')\n",
    "    return y_inp\n",
    "# If you saved the complete model, you can load it directly\n",
    "#caption_model = tf.keras.models.load_model('Saved Models/image_captioning_transformer_model.h5')\n",
    "\n",
    "# Now you can use the model for predictions\n",
    "# Example: predictions = caption_model.predict(input_data)\n",
    "\n",
    "\n",
    "# Example input data (make sure it matches the input shape of your model)\n",
    "input_data ='test_for_fashion/m11.jpg'# Replace with actual input data\n",
    "\n",
    "\n",
    "pred_caption = generate_caption('test_for_fashion/m11.jpg')\n",
    "print('Predicted Caption:', pred_caption)\n",
    "print()\n",
    "#im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_caption = generate_caption('test_for_fashion/m11.jpg')\n",
    "print('Predicted Caption:', pred_caption)\n",
    "print()\n",
    "#im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
