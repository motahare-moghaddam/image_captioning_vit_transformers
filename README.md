# image_captioning_vit_transformers

ðŸ§  Image Captioning with Transformers on Fashion Dataset ðŸ‘—ðŸ§¢ðŸ‘ 

This project implements an image captioning model that automatically generates natural-language descriptions for fashion product images using Transformers and InceptionV3 CNN encoders.
The dataset is sourced from the Fashion Product Images Dataset on Kaggle, which includes thousands of fashion items with attributes like gender, category, color, season, and usage.

ðŸ“¸ Example Outputs
Input Image	Generated Caption

![Image](https://github.com/user-attachments/assets/b12ab4f4-dafc-4823-afa7-78b3b789cfcc)

Predicted Caption: start women apparel topwear tshirts pink summer casual myntra women pink tshirt end







![Image](https://github.com/user-attachments/assets/2dd82164-1c2a-4557-a210-81734124a6d8)

Predicted Caption: start men footwear shoes sports shoes white summer sports nike men air impetus white sports shoes end



ðŸš€ Project Overview

The model combines:

CNN Encoder (InceptionV3) â†’ Extracts visual features.

Transformer Encoderâ€“Decoder â†’ Learns contextual relationships between image embeddings and word sequences.

Tokenizer (TextVectorization) â†’ Handles vocabulary and sequence mapping.

The output is a caption describing fashion attributes such as gender, clothing type, color, and season.

ðŸ§© Architecture
Image â†’ InceptionV3 CNN â†’ Transformer Encoder â†’ Transformer Decoder â†’ Caption


Encoder: InceptionV3 pretrained on ImageNet (frozen feature extractor)

Decoder: Transformer with Multi-Head Attention & Positional Embeddings

Loss: Sparse Categorical Crossentropy

Optimizer: Adam

Evaluation Metric: Token-level Accuracy

ðŸ“Š Dataset

Source: Fashion Product Images (Small) - Kaggle

Size: ~44,000 images

Labels: gender, masterCategory, subCategory, articleType, baseColour, season, usage, productDisplayName

Each image caption was generated by combining multiple metadata fields, cleaned, and tokenized into natural sentences.


