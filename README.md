# image_captioning_vit_transformers

🧠 Image Captioning with Transformers on Fashion Dataset 👗🧢👠

This project implements an image captioning model that automatically generates natural-language descriptions for fashion product images using Transformers and InceptionV3 CNN encoders.
The dataset is sourced from the Fashion Product Images Dataset on Kaggle, which includes thousands of fashion items with attributes like gender, category, color, season, and usage.

📸 Example Outputs
Input Image	Generated Caption

	start men footwear shoes sports shoes white summer sports nike men air impetus white sports shoes end

	start women accessories jewellery jewellery set gold winter casual estelle women gold jewellery set end
🚀 Project Overview

The model combines:

CNN Encoder (InceptionV3) → Extracts visual features.

Transformer Encoder–Decoder → Learns contextual relationships between image embeddings and word sequences.

Tokenizer (TextVectorization) → Handles vocabulary and sequence mapping.

The output is a caption describing fashion attributes such as gender, clothing type, color, and season.

🧩 Architecture
Image → InceptionV3 CNN → Transformer Encoder → Transformer Decoder → Caption


Encoder: InceptionV3 pretrained on ImageNet (frozen feature extractor)

Decoder: Transformer with Multi-Head Attention & Positional Embeddings

Loss: Sparse Categorical Crossentropy

Optimizer: Adam

Evaluation Metric: Token-level Accuracy

📊 Dataset

Source: Fashion Product Images (Small) - Kaggle

Size: ~44,000 images

Labels: gender, masterCategory, subCategory, articleType, baseColour, season, usage, productDisplayName

Each image caption was generated by combining multiple metadata fields, cleaned, and tokenized into natural sentences.

⚙️ Installation & Setup
1️⃣ Clone the Repository
git clone https://github.com/YOUR_USERNAME/fashion-image-captioning-transformer.git
cd fashion-image-captioning-transformer
